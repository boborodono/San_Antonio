{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning \"Stroke Prediction Dataset\" (Parquet) with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime\n",
    "import time\n",
    "start = time.time()\n",
    "#* BEGINS TIMER ^^\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Read Parquet Dataset into Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#? Load File\n",
    "Path = \"1_parquet_conversion/stroke.parquet.gzip\"\n",
    "\n",
    "#? Read the CSVs into a dataframe\n",
    "stroke_df = pd.read_parquet(Path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to the smallest datatype possible for each numeric column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? FLOATS\n",
    "# float_cols = stroke_df.select_dtypes(include=['float'])\n",
    "\n",
    "# for cols in float_cols.columns:\n",
    "#     stroke_df[cols] = pd.to_numeric(stroke_df[cols], downcast = 'float')\n",
    "\n",
    "#! I commented out the above because it was causing the decimal places to extend and I couldn't use \"np.round\" to round back to 1 or 2 decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? INTEGERS\n",
    "int_cols = stroke_df.select_dtypes(include=['int'])\n",
    "\n",
    "for cols in int_cols.columns:\n",
    "    stroke_df[cols] = pd.to_numeric(stroke_df[cols], downcast = 'integer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Changed Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manully Convert 'age', 'avg_glucose_level', & 'bmi' columns to smallest datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTEGERS\n",
    "stroke_df['age'] = stroke_df['age'].astype('int8')\n",
    "\n",
    "# FLOATS\n",
    "# stroke_df['avg_glucose_level'] = stroke_df['avg_glucose_level'].astype('float32')\n",
    "# stroke_df['bmi'] = stroke_df['bmi'].astype('float32')\n",
    "\n",
    "#! It doesn't work this way either!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the Shape of the DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### isnull Method for 'bmi' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_isnull_df = stroke_df['bmi'].isnull().sum()\n",
    "stroke_isnull_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If I dropped all these rows, what percentage of the data would be lost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the average of the DataFrame isna(){bool}\n",
    "# Round to 4 numbers\n",
    "# Multiply by 100 to get %\n",
    "column_missing_percent = stroke_df['bmi'].isna().mean().round(4) * 100\n",
    "column_missing_percent\n",
    "print('------------------------------------------------------')\n",
    "print(f'The # of missing values in the \"bmi\" column is {stroke_isnull_df}')\n",
    "print('--------------------------AND-------------------------')\n",
    "print(f'{column_missing_percent}% of the column is missing values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is this an acceptable percentage of ROWS to delete?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! ASK THE GROUP!!! or USE YOUR STATS SKILLS!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Missing Rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stroke_df = stroke_df.dropna(subset=['bmi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do we want to delete any Columns? (I don't think so)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! I think we can delete the ID column if we reset the index to be the new ID column and start it at \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stroke_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename columns before resetting index to new \"ID\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the index to be the \"ID\" column before resetting the index\n",
    "stroke_df.columns = ['ID',\n",
    "                    'Gender',\n",
    "                    'Age',\n",
    "                    'Hypertension',\n",
    "                    'Heart Disease',\n",
    "                    'Ever Married',\n",
    "                    'Work Type',\n",
    "                    'Residence Type',\n",
    "                    'Avg Glucose Lvl',\n",
    "                    'BMI',\n",
    "                    'Smoker',\n",
    "                    'Stroke',\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the index to start at '1' and set as the new 'ID' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_df['ID'] = stroke_df.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Index\n",
    "stroke_df = stroke_df.set_index('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stroke_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reorder Columns???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List column names\n",
    "stroke_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# new_column_order = ['Gender',\n",
    "#                     'Age',\n",
    "#                     'Hypertension',\n",
    "#                     'Heart Disease',\n",
    "#                     'Ever Married',\n",
    "#                     'Work Type',\n",
    "#                     'Residence Type',\n",
    "#                     'Avg Glucose Lvl',\n",
    "#                     'BMI',\n",
    "#                     'Smoker',\n",
    "#                     'Stroke',\n",
    "#                     ]\n",
    "\n",
    "# stroke_df = stroke_df[new_column_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change responses for \"Work Type\" & \"Smoker\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_df['Smoker'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_df= stroke_df.replace({\"Smoker\":'formerly smoked'}, \"Former\")\n",
    "stroke_df= stroke_df.replace({\"Smoker\": 'never smoked'}, \"Never\")\n",
    "stroke_df= stroke_df.replace({\"Smoker\":'smokes'}, \"Current\")          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_df['Work Type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_df= stroke_df.replace({\"Work Type\":'Govt_job'}, \"Government\")\n",
    "stroke_df= stroke_df.replace({\"Work Type\": 'children'}, \"Child\")\n",
    "stroke_df= stroke_df.replace({\"Work Type\":'Self-employed'}, \"Self-Employed\")\n",
    "stroke_df= stroke_df.replace({\"Work Type\":'Never_worked'}, \"Never Worked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Cleaned DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huzzah!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Parquet File to Resources Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Parquet File\n",
    "stroke_df.to_parquet('..\\..\\Resources\\Cleaned_Dataset\\clean_stroke.parquet.gzip', compression='gzip', index=False)\n",
    "\n",
    "# Export CSV File\n",
    "stroke_df.to_csv('..\\..\\Resources\\Cleaned_Dataset\\clean_stroke.csv', index=False)\n",
    "\n",
    "\n",
    "#* ENDS TIMER\n",
    "end = time.time()\n",
    "print(f'{end - start:.2f} seconds')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "292c63eb52e1efa1c2f4da13901b793a21615828fa9ca98854863b7c3760bba5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('mlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
